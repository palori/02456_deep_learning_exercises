{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "This is heavily influenced by [hedgehoglabs/xor](https://github.com/hedgehoglabs/xor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks (RNN)\n",
    "\n",
    "This notebook introduces the concept of Recurrent Neural Networks (RNN).\n",
    "You won't have as many programming tasks in this exercise, but do make sure that you read everyhing carefully, and understand what is going on.\n",
    "\n",
    "___\n",
    "\n",
    "A recurrent neural network (RNN) is a type of neural network that has been succesful in modelling sequential data, e.g. language, speech, protein sequences, etc.\n",
    "\n",
    "A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.\n",
    "The idea is that the network should be able to use the previous computations as some form of memory and apply this to the future computation.\n",
    "An image may best explain how this is to be understood,\n",
    "\n",
    "![rnn-unroll image](../static_files/rnn-unfold.png)\n",
    "\n",
    "\n",
    "where it the network contains the following elements:\n",
    "\n",
    "- $x$ is the input sequence of samples, \n",
    "- $U$ is a weight matrix applied to the given input sample,\n",
    "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\n",
    "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),\n",
    "- $h$ is the hidden state (the network's memory) for a given time step, and\n",
    "- $o$ is the resulting output.\n",
    "\n",
    "When the network is unrolled as shown, it is easier to refer to a timestep, $t$.\n",
    "We have the following computations through the network:\n",
    "\n",
    "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ usually is an activation function, e.g. $\\mathrm{tanh}$.\n",
    "- $o_t = \\mathrm{softmax}(W\\,{h_t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem of Parity\n",
    "(parity simply means wheather the number is even or odd)\n",
    "\n",
    "The exercise comes from the OpenAI [Requests for Research 2.0](https://blog.openai.com/requests-for-research-2/) warmup exercise.\n",
    "The exercise reads:\n",
    "\n",
    "> Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequenceâ€™s end. Test the two approaches below:\n",
    "> 1. Generate a dataset of random 100,000 binary strings of length 50. Train the LSTM; what performance do you get?\n",
    "> 1. Generate a dataset of random 100,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?\n",
    "\n",
    "LSTM stands for 'Long short-term memory', and is an extension of the standard RNN model described above.\n",
    "An LSTM has been extended with a memory module that enables it to remember over longer sequences.\n",
    "\n",
    "In this exercise we will however **stick with a standard RNN instead of an LSTM**.\n",
    "(The LSTM model will be covered later. See [this excelent blogpost](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah for more on LSTM's.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the Data\n",
    "\n",
    "Before we do much of anything we need some data to work with.\n",
    "The code below we generate the binary vectors that we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils import rnn as rnn_utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "max_bits = 50\n",
    "batch_size = 8\n",
    "\n",
    "DEFAULT_NUM_BITS = 50\n",
    "DEFAULT_NUM_SEQUENCES = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to load the data set\n",
    "\n",
    "class XORDataset(data.Dataset):\n",
    "    def __init__(self, num_sequences=DEFAULT_NUM_SEQUENCES, num_bits=DEFAULT_NUM_BITS):\n",
    "        self.num_sequences = num_sequences\n",
    "        self.num_bits = num_bits\n",
    "\n",
    "        self.features, self.labels = get_random_bits_parity(num_sequences, num_bits)\n",
    "\n",
    "        # expand the dimensions for the rnn\n",
    "        # [batch, bits] -> [batch, bits, 1]\n",
    "        self.features = np.expand_dims(self.features, -1)\n",
    "\n",
    "        # [batch, parity] -> [batch, parity, 1]\n",
    "        self.labels = np.expand_dims(self.labels, -1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index, :], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "def get_random_bits_parity(num_sequences, num_bits):\n",
    "    \"\"\"Generate random bit sequences and their parity. (Our features and labels).\n",
    "        Returns:\n",
    "            bit_sequences: A numpy array of bit sequences with shape [num_sequences, num_bits].\n",
    "            parity: A numpy array of even parity values corresponding to each bit\n",
    "                with shape [num_sequences, num_bits].\n",
    "        \"\"\"\n",
    "    bit_sequences = np.random.randint(2, size=(num_sequences, num_bits))\n",
    "\n",
    "    # if total number of ones is odd, set even parity bit to 1, otherwise 0\n",
    "    # https://en.wikipedia.org/wiki/Parity_bit\n",
    "\n",
    "#     bitsum = np.sum(bit_sequences, axis=1)  # use only the final result.\n",
    "    bitsum = np.cumsum(bit_sequences, axis=1)  # Teacher forcing\n",
    "\n",
    "    # if bitsum is even: False, odd: True\n",
    "    parity = bitsum % 2 != 0\n",
    "\n",
    "    return bit_sequences.astype('float32'), parity.astype('float32')\n",
    "\n",
    "train_loader = DataLoader(XORDataset(num_bits=max_bits), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what it looks like.\n",
    "In order to make the print statement look nicer we print the transpose of the data - i.e. the data is `[seq_len, num_seq]` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "[[[0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "  [1. 1. 1. 1. 0. 0. 1. 1.]\n",
      "  [0. 0. 1. 0. 1. 1. 1. 1.]\n",
      "  [1. 1. 0. 0. 1. 1. 0. 1.]\n",
      "  [1. 0. 1. 1. 0. 0. 0. 1.]\n",
      "  [1. 1. 0. 1. 0. 1. 0. 1.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [1. 0. 1. 1. 0. 0. 1. 1.]\n",
      "  [0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "  [0. 1. 1. 0. 0. 0. 1. 0.]\n",
      "  [1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 1.]\n",
      "  [1. 1. 1. 0. 1. 1. 1. 0.]\n",
      "  [1. 1. 0. 1. 0. 1. 0. 1.]]]\n",
      "\n",
      "targets\n",
      "[[[0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "  [1. 0. 1. 0. 0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0. 1. 1. 0. 1.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "  [1. 0. 1. 1. 0. 1. 1. 0.]\n",
      "  [1. 0. 0. 0. 1. 0. 0. 1.]\n",
      "  [1. 1. 1. 0. 1. 0. 1. 1.]\n",
      "  [0. 0. 1. 1. 1. 0. 1. 1.]\n",
      "  [0. 1. 1. 1. 1. 0. 1. 0.]\n",
      "  [1. 0. 0. 1. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets  in train_loader:\n",
    "    print('inputs')\n",
    "    print(inputs.numpy()[:,:15].T)\n",
    "#     print(np.sum(inputs.numpy(), 1).T)\n",
    "    print()\n",
    "    print('targets')\n",
    "    print(targets.numpy()[:,:15].T)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than only have a target for the very last element in the sequence, we make a target for every sub-sequence along the way.\n",
    "During training we use all these targets to help the network learn along the way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN Model\n",
    "\n",
    "One of the nice things with using a library like `PyTorch` is that a lot of the complexity is hidden away.\n",
    "Creating a RNN is therefore not much different than creating any other kind of network, as we shall see below.\n",
    "Do however make sure to understand the concepts - they will help you debug, or when you want to work with more advanced concepts.\n",
    "\n",
    "Things to note however:\n",
    " * `batch_first=True` determines the order of the sequence. Normally in `PyTorch` the input is expected to be `[seq_len, batch, input_size]`, but this is sometimes inconvenient, so we use `batch_first=True` to make the input `[batch, seq, feature]`. \n",
    " * `pack_padded_sequence` and `pad_packed_sequence` are used for variable length sequences when the sequences don't have the same length. (Computers don't deal well with sequences of varing length, so we use padding to make them align).\n",
    " \n",
    "For more on RNNs in `PyTorch` [see here](https://pytorch.org/docs/stable/nn.html#torch.nn.RNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = torch.nn.RNN(\n",
    "                batch_first=True,\n",
    "                input_size=1,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=1)\n",
    "\n",
    "        self.hidden_to_logits = torch.nn.Linear(hidden_size, 1)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # pack the inputs\n",
    "        packed_inputs = rnn_utils.pack_padded_sequence(\n",
    "                inputs, lengths, batch_first=True).to(device)\n",
    "\n",
    "        rnn_out, _ = self.rnn(packed_inputs)\n",
    "\n",
    "        unpacked, _ = rnn_utils.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "\n",
    "        logits = self.hidden_to_logits(unpacked)\n",
    "        predictions = self.activation(logits)\n",
    "\n",
    "        return logits, predictions\n",
    "\n",
    "hidden_size = 4\n",
    "\n",
    "model = RNN(hidden_size, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As allways we need to define our optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "#lr = 1\n",
    "#momentum = 0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions - you don't need to worry about them.\n",
    "\n",
    "def adjust_lengths(vary_lengths, inputs, targets):\n",
    "    batch_size = inputs.size()[0]\n",
    "    max_bits = inputs.size()[1]\n",
    "\n",
    "    if not vary_lengths:\n",
    "        lengths = torch.ones(batch_size, dtype=torch.int) * max_bits\n",
    "        return lengths\n",
    "\n",
    "    # choose random lengths\n",
    "    lengths = np.random.randint(1, max_bits, size=batch_size, dtype=int)\n",
    "\n",
    "    # keep one the max size so we don't need to resize targets for the loss\n",
    "    lengths[0] = max_bits\n",
    "\n",
    "    # sort in descending order\n",
    "    lengths = -np.sort(-lengths)\n",
    "\n",
    "    # chop the bits based on lengths\n",
    "    for i, sample_length in enumerate(lengths):\n",
    "        inputs[i, lengths[i]:, ] = 0\n",
    "        targets[i, lengths[i]:, ] = 0\n",
    "\n",
    "    return lengths\n",
    "\n",
    "\n",
    "def evaluate(max_bits, vary_lengths, device, model):\n",
    "    # evaluate on more bits than training to ensure generalization\n",
    "    valid_loader = DataLoader(\n",
    "            XORDataset(num_sequences=5000, num_bits=int(max_bits * 1.5)), batch_size=500)\n",
    "\n",
    "    is_correct = np.array([])\n",
    "\n",
    "    for inputs, targets in valid_loader:\n",
    "        lengths = adjust_lengths(vary_lengths, inputs, targets)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, predictions = model(inputs, lengths)\n",
    "            is_correct = np.append(is_correct, ((predictions > 0.5) == (targets > 0.5)))\n",
    "\n",
    "    accuracy = is_correct.mean()\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "We are now ready to train the network. \n",
    "This is also very similar to what we have already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1, step   250. Train summary: loss 0.6932, accuracy 0.498\n",
      "epoch  1, step   500. Train summary: loss 0.6934, accuracy 0.510\n",
      "epoch  1, step   750. Train summary: loss 0.6932, accuracy 0.502\n",
      "epoch  1, step  1000. Train summary: loss 0.6925, accuracy 0.555\n",
      "Validation accuracy 0.499\n",
      "\n",
      "epoch  1, step  1250. Train summary: loss 0.6929, accuracy 0.527\n",
      "epoch  1, step  1500. Train summary: loss 0.6938, accuracy 0.490\n",
      "epoch  1, step  1750. Train summary: loss 0.6935, accuracy 0.505\n",
      "epoch  1, step  2000. Train summary: loss 0.6937, accuracy 0.500\n",
      "Validation accuracy 0.508\n",
      "\n",
      "epoch  1, step  2250. Train summary: loss 0.6927, accuracy 0.510\n",
      "epoch  1, step  2500. Train summary: loss 0.6935, accuracy 0.500\n",
      "epoch  1, step  2750. Train summary: loss 0.6956, accuracy 0.477\n",
      "epoch  1, step  3000. Train summary: loss 0.6936, accuracy 0.480\n",
      "Validation accuracy 0.506\n",
      "\n",
      "epoch  1, step  3250. Train summary: loss 0.6936, accuracy 0.493\n",
      "epoch  1, step  3500. Train summary: loss 0.6922, accuracy 0.535\n",
      "epoch  1, step  3750. Train summary: loss 0.6938, accuracy 0.475\n",
      "epoch  1, step  4000. Train summary: loss 0.6917, accuracy 0.533\n",
      "Validation accuracy 0.508\n",
      "\n",
      "epoch  1, step  4250. Train summary: loss 0.6662, accuracy 0.543\n",
      "epoch  1, step  4500. Train summary: loss 0.6628, accuracy 0.525\n",
      "epoch  1, step  4750. Train summary: loss 0.6576, accuracy 0.585\n",
      "epoch  1, step  5000. Train summary: loss 0.6697, accuracy 0.512\n",
      "Validation accuracy 0.508\n",
      "\n",
      "epoch  1, step  5250. Train summary: loss 0.6649, accuracy 0.507\n",
      "epoch  1, step  5500. Train summary: loss 0.6632, accuracy 0.490\n",
      "epoch  1, step  5750. Train summary: loss 0.6581, accuracy 0.477\n",
      "epoch  1, step  6000. Train summary: loss 0.6537, accuracy 0.550\n",
      "Validation accuracy 0.519\n",
      "\n",
      "epoch  1, step  6250. Train summary: loss 0.6617, accuracy 0.472\n",
      "epoch  1, step  6500. Train summary: loss 0.6627, accuracy 0.507\n",
      "epoch  1, step  6750. Train summary: loss 0.6324, accuracy 0.613\n",
      "epoch  1, step  7000. Train summary: loss 0.6567, accuracy 0.565\n",
      "Validation accuracy 0.534\n",
      "\n",
      "epoch  1, step  7250. Train summary: loss 0.6628, accuracy 0.498\n",
      "epoch  1, step  7500. Train summary: loss 0.6616, accuracy 0.498\n",
      "epoch  1, step  7750. Train summary: loss 0.6827, accuracy 0.522\n",
      "epoch  1, step  8000. Train summary: loss 0.6839, accuracy 0.477\n",
      "Validation accuracy 0.499\n",
      "\n",
      "epoch  1, step  8250. Train summary: loss 0.6779, accuracy 0.488\n",
      "epoch  1, step  8500. Train summary: loss 0.6792, accuracy 0.505\n",
      "epoch  1, step  8750. Train summary: loss 0.6877, accuracy 0.527\n",
      "epoch  1, step  9000. Train summary: loss 0.6695, accuracy 0.562\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  1, step  9250. Train summary: loss 0.6835, accuracy 0.480\n",
      "epoch  1, step  9500. Train summary: loss 0.6790, accuracy 0.495\n",
      "epoch  1, step  9750. Train summary: loss 0.6864, accuracy 0.495\n",
      "epoch  1, step 10000. Train summary: loss 0.6735, accuracy 0.580\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  1, step 10250. Train summary: loss 0.6886, accuracy 0.507\n",
      "epoch  1, step 10500. Train summary: loss 0.6730, accuracy 0.510\n",
      "epoch  1, step 10750. Train summary: loss 0.6858, accuracy 0.520\n",
      "epoch  1, step 11000. Train summary: loss 0.6788, accuracy 0.555\n",
      "Validation accuracy 0.512\n",
      "\n",
      "epoch  1, step 11250. Train summary: loss 0.6844, accuracy 0.517\n",
      "epoch  1, step 11500. Train summary: loss 0.6831, accuracy 0.545\n",
      "epoch  1, step 11750. Train summary: loss 0.6788, accuracy 0.530\n",
      "epoch  1, step 12000. Train summary: loss 0.6804, accuracy 0.527\n",
      "Validation accuracy 0.500\n",
      "\n",
      "epoch  1, step 12250. Train summary: loss 0.6759, accuracy 0.488\n",
      "epoch  1, step 12500. Train summary: loss 0.6691, accuracy 0.510\n",
      "epoch  2, step 12750. Train summary: loss 0.6864, accuracy 0.498\n",
      "epoch  2, step 13000. Train summary: loss 0.6810, accuracy 0.530\n",
      "Validation accuracy 0.499\n",
      "\n",
      "epoch  2, step 13250. Train summary: loss 0.6741, accuracy 0.520\n",
      "epoch  2, step 13500. Train summary: loss 0.6674, accuracy 0.565\n",
      "epoch  2, step 13750. Train summary: loss 0.6784, accuracy 0.527\n",
      "epoch  2, step 14000. Train summary: loss 0.6859, accuracy 0.500\n",
      "Validation accuracy 0.505\n",
      "\n",
      "epoch  2, step 14250. Train summary: loss 0.6798, accuracy 0.505\n",
      "epoch  2, step 14500. Train summary: loss 0.6833, accuracy 0.488\n",
      "epoch  2, step 14750. Train summary: loss 0.6644, accuracy 0.558\n",
      "epoch  2, step 15000. Train summary: loss 0.6822, accuracy 0.452\n",
      "Validation accuracy 0.525\n",
      "\n",
      "epoch  2, step 15250. Train summary: loss 0.6722, accuracy 0.565\n",
      "epoch  2, step 15500. Train summary: loss 0.6882, accuracy 0.533\n",
      "epoch  2, step 15750. Train summary: loss 0.6825, accuracy 0.507\n",
      "epoch  2, step 16000. Train summary: loss 0.6737, accuracy 0.540\n",
      "Validation accuracy 0.518\n",
      "\n",
      "epoch  2, step 16250. Train summary: loss 0.6889, accuracy 0.430\n",
      "epoch  2, step 16500. Train summary: loss 0.6649, accuracy 0.550\n",
      "epoch  2, step 16750. Train summary: loss 0.6757, accuracy 0.550\n",
      "epoch  2, step 17000. Train summary: loss 0.6805, accuracy 0.502\n",
      "Validation accuracy 0.501\n",
      "\n",
      "epoch  2, step 17250. Train summary: loss 0.6840, accuracy 0.545\n",
      "epoch  2, step 17500. Train summary: loss 0.6783, accuracy 0.482\n",
      "epoch  2, step 17750. Train summary: loss 0.6802, accuracy 0.553\n",
      "epoch  2, step 18000. Train summary: loss 0.6790, accuracy 0.498\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  2, step 18250. Train summary: loss 0.6804, accuracy 0.522\n",
      "epoch  2, step 18500. Train summary: loss 0.6735, accuracy 0.530\n",
      "epoch  2, step 18750. Train summary: loss 0.6799, accuracy 0.520\n",
      "epoch  2, step 19000. Train summary: loss 0.6857, accuracy 0.553\n",
      "Validation accuracy 0.517\n",
      "\n",
      "epoch  2, step 19250. Train summary: loss 0.6676, accuracy 0.592\n",
      "epoch  2, step 19500. Train summary: loss 0.6832, accuracy 0.470\n",
      "epoch  2, step 19750. Train summary: loss 0.6701, accuracy 0.515\n",
      "epoch  2, step 20000. Train summary: loss 0.6902, accuracy 0.488\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  2, step 20250. Train summary: loss 0.6723, accuracy 0.500\n",
      "epoch  2, step 20500. Train summary: loss 0.6951, accuracy 0.530\n",
      "epoch  2, step 20750. Train summary: loss 0.6905, accuracy 0.535\n",
      "epoch  2, step 21000. Train summary: loss 0.6609, accuracy 0.527\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  2, step 21250. Train summary: loss 0.6862, accuracy 0.505\n",
      "epoch  2, step 21500. Train summary: loss 0.6849, accuracy 0.507\n",
      "epoch  2, step 21750. Train summary: loss 0.6711, accuracy 0.580\n",
      "epoch  2, step 22000. Train summary: loss 0.6838, accuracy 0.515\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  2, step 22250. Train summary: loss 0.6704, accuracy 0.512\n",
      "epoch  2, step 22500. Train summary: loss 0.6696, accuracy 0.562\n",
      "epoch  2, step 22750. Train summary: loss 0.6764, accuracy 0.498\n",
      "epoch  2, step 23000. Train summary: loss 0.6655, accuracy 0.560\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  2, step 23250. Train summary: loss 0.6666, accuracy 0.520\n",
      "epoch  2, step 23500. Train summary: loss 0.6573, accuracy 0.575\n",
      "epoch  2, step 23750. Train summary: loss 0.6585, accuracy 0.540\n",
      "epoch  2, step 24000. Train summary: loss 0.6660, accuracy 0.500\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  2, step 24250. Train summary: loss 0.6605, accuracy 0.522\n",
      "epoch  2, step 24500. Train summary: loss 0.6721, accuracy 0.530\n",
      "epoch  2, step 24750. Train summary: loss 0.6658, accuracy 0.520\n",
      "epoch  2, step 25000. Train summary: loss 0.6654, accuracy 0.505\n",
      "Validation accuracy 0.515\n",
      "\n",
      "epoch  3, step 25250. Train summary: loss 0.6607, accuracy 0.493\n",
      "epoch  3, step 25500. Train summary: loss 0.6698, accuracy 0.505\n",
      "epoch  3, step 25750. Train summary: loss 0.6640, accuracy 0.520\n",
      "epoch  3, step 26000. Train summary: loss 0.6661, accuracy 0.495\n",
      "Validation accuracy 0.511\n",
      "\n",
      "epoch  3, step 26250. Train summary: loss 0.6716, accuracy 0.517\n",
      "epoch  3, step 26500. Train summary: loss 0.6640, accuracy 0.485\n",
      "epoch  3, step 26750. Train summary: loss 0.6637, accuracy 0.522\n",
      "epoch  3, step 27000. Train summary: loss 0.6683, accuracy 0.520\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  3, step 27250. Train summary: loss 0.6649, accuracy 0.540\n",
      "epoch  3, step 27500. Train summary: loss 0.6674, accuracy 0.515\n",
      "epoch  3, step 27750. Train summary: loss 0.6748, accuracy 0.510\n",
      "epoch  3, step 28000. Train summary: loss 0.6571, accuracy 0.522\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  3, step 28250. Train summary: loss 0.6675, accuracy 0.510\n",
      "epoch  3, step 28500. Train summary: loss 0.6677, accuracy 0.565\n",
      "epoch  3, step 28750. Train summary: loss 0.6664, accuracy 0.543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  3, step 29000. Train summary: loss 0.6741, accuracy 0.500\n",
      "Validation accuracy 0.512\n",
      "\n",
      "epoch  3, step 29250. Train summary: loss 0.6598, accuracy 0.560\n",
      "epoch  3, step 29500. Train summary: loss 0.6743, accuracy 0.517\n",
      "epoch  3, step 29750. Train summary: loss 0.6729, accuracy 0.463\n",
      "epoch  3, step 30000. Train summary: loss 0.6617, accuracy 0.470\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  3, step 30250. Train summary: loss 0.6724, accuracy 0.505\n",
      "epoch  3, step 30500. Train summary: loss 0.6765, accuracy 0.517\n",
      "epoch  3, step 30750. Train summary: loss 0.6529, accuracy 0.545\n",
      "epoch  3, step 31000. Train summary: loss 0.6601, accuracy 0.533\n",
      "Validation accuracy 0.512\n",
      "\n",
      "epoch  3, step 31250. Train summary: loss 0.6712, accuracy 0.522\n",
      "epoch  3, step 31500. Train summary: loss 0.6510, accuracy 0.548\n",
      "epoch  3, step 31750. Train summary: loss 0.6585, accuracy 0.530\n",
      "epoch  3, step 32000. Train summary: loss 0.6735, accuracy 0.543\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  3, step 32250. Train summary: loss 0.6659, accuracy 0.490\n",
      "epoch  3, step 32500. Train summary: loss 0.6660, accuracy 0.567\n",
      "epoch  3, step 32750. Train summary: loss 0.6688, accuracy 0.525\n",
      "epoch  3, step 33000. Train summary: loss 0.6768, accuracy 0.502\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  3, step 33250. Train summary: loss 0.6698, accuracy 0.490\n",
      "epoch  3, step 33500. Train summary: loss 0.6671, accuracy 0.530\n",
      "epoch  3, step 33750. Train summary: loss 0.6780, accuracy 0.498\n",
      "epoch  3, step 34000. Train summary: loss 0.6672, accuracy 0.522\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  3, step 34250. Train summary: loss 0.6608, accuracy 0.490\n",
      "epoch  3, step 34500. Train summary: loss 0.6744, accuracy 0.507\n",
      "epoch  3, step 34750. Train summary: loss 0.6603, accuracy 0.530\n",
      "epoch  3, step 35000. Train summary: loss 0.6758, accuracy 0.480\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  3, step 35250. Train summary: loss 0.6778, accuracy 0.517\n",
      "epoch  3, step 35500. Train summary: loss 0.6706, accuracy 0.522\n",
      "epoch  3, step 35750. Train summary: loss 0.6792, accuracy 0.465\n",
      "epoch  3, step 36000. Train summary: loss 0.6758, accuracy 0.545\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  3, step 36250. Train summary: loss 0.6616, accuracy 0.548\n",
      "epoch  3, step 36500. Train summary: loss 0.6606, accuracy 0.520\n",
      "epoch  3, step 36750. Train summary: loss 0.6685, accuracy 0.527\n",
      "epoch  3, step 37000. Train summary: loss 0.6703, accuracy 0.540\n",
      "Validation accuracy 0.515\n",
      "\n",
      "epoch  3, step 37250. Train summary: loss 0.6784, accuracy 0.515\n",
      "epoch  3, step 37500. Train summary: loss 0.6627, accuracy 0.517\n",
      "epoch  4, step 37750. Train summary: loss 0.6603, accuracy 0.517\n",
      "epoch  4, step 38000. Train summary: loss 0.6620, accuracy 0.522\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  4, step 38250. Train summary: loss 0.6653, accuracy 0.527\n",
      "epoch  4, step 38500. Train summary: loss 0.6623, accuracy 0.507\n",
      "epoch  4, step 38750. Train summary: loss 0.6696, accuracy 0.570\n",
      "epoch  4, step 39000. Train summary: loss 0.6615, accuracy 0.520\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  4, step 39250. Train summary: loss 0.6644, accuracy 0.505\n",
      "epoch  4, step 39500. Train summary: loss 0.6654, accuracy 0.540\n",
      "epoch  4, step 39750. Train summary: loss 0.6804, accuracy 0.460\n",
      "epoch  4, step 40000. Train summary: loss 0.6674, accuracy 0.490\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  4, step 40250. Train summary: loss 0.6683, accuracy 0.493\n",
      "epoch  4, step 40500. Train summary: loss 0.6641, accuracy 0.465\n",
      "epoch  4, step 40750. Train summary: loss 0.6607, accuracy 0.507\n",
      "epoch  4, step 41000. Train summary: loss 0.6709, accuracy 0.510\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  4, step 41250. Train summary: loss 0.6750, accuracy 0.485\n",
      "epoch  4, step 41500. Train summary: loss 0.6704, accuracy 0.527\n",
      "epoch  4, step 41750. Train summary: loss 0.6671, accuracy 0.527\n",
      "epoch  4, step 42000. Train summary: loss 0.6634, accuracy 0.472\n",
      "Validation accuracy 0.512\n",
      "\n",
      "epoch  4, step 42250. Train summary: loss 0.6756, accuracy 0.468\n",
      "epoch  4, step 42500. Train summary: loss 0.6722, accuracy 0.520\n",
      "epoch  4, step 42750. Train summary: loss 0.6690, accuracy 0.522\n",
      "epoch  4, step 43000. Train summary: loss 0.6619, accuracy 0.530\n",
      "Validation accuracy 0.512\n",
      "\n",
      "epoch  4, step 43250. Train summary: loss 0.6649, accuracy 0.507\n",
      "epoch  4, step 43500. Train summary: loss 0.6482, accuracy 0.535\n",
      "epoch  4, step 43750. Train summary: loss 0.6668, accuracy 0.533\n",
      "epoch  4, step 44000. Train summary: loss 0.6616, accuracy 0.498\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  4, step 44250. Train summary: loss 0.6797, accuracy 0.498\n",
      "epoch  4, step 44500. Train summary: loss 0.6539, accuracy 0.515\n",
      "epoch  4, step 44750. Train summary: loss 0.6677, accuracy 0.507\n",
      "epoch  4, step 45000. Train summary: loss 0.6717, accuracy 0.535\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  4, step 45250. Train summary: loss 0.6655, accuracy 0.517\n",
      "epoch  4, step 45500. Train summary: loss 0.6608, accuracy 0.533\n",
      "epoch  4, step 45750. Train summary: loss 0.6705, accuracy 0.525\n",
      "epoch  4, step 46000. Train summary: loss 0.6741, accuracy 0.493\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  4, step 46250. Train summary: loss 0.6723, accuracy 0.522\n",
      "epoch  4, step 46500. Train summary: loss 0.6529, accuracy 0.600\n",
      "epoch  4, step 46750. Train summary: loss 0.6760, accuracy 0.485\n",
      "epoch  4, step 47000. Train summary: loss 0.6706, accuracy 0.517\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  4, step 47250. Train summary: loss 0.6704, accuracy 0.502\n",
      "epoch  4, step 47500. Train summary: loss 0.6728, accuracy 0.498\n",
      "epoch  4, step 47750. Train summary: loss 0.6724, accuracy 0.517\n",
      "epoch  4, step 48000. Train summary: loss 0.6724, accuracy 0.525\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  4, step 48250. Train summary: loss 0.6733, accuracy 0.520\n",
      "epoch  4, step 48500. Train summary: loss 0.6661, accuracy 0.507\n",
      "epoch  4, step 48750. Train summary: loss 0.6695, accuracy 0.540\n",
      "epoch  4, step 49000. Train summary: loss 0.6657, accuracy 0.515\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  4, step 49250. Train summary: loss 0.6691, accuracy 0.560\n",
      "epoch  4, step 49500. Train summary: loss 0.6733, accuracy 0.475\n",
      "epoch  4, step 49750. Train summary: loss 0.6681, accuracy 0.540\n",
      "epoch  4, step 50000. Train summary: loss 0.6645, accuracy 0.493\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  5, step 50250. Train summary: loss 0.6705, accuracy 0.525\n",
      "epoch  5, step 50500. Train summary: loss 0.6642, accuracy 0.522\n",
      "epoch  5, step 50750. Train summary: loss 0.6735, accuracy 0.488\n",
      "epoch  5, step 51000. Train summary: loss 0.6538, accuracy 0.590\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  5, step 51250. Train summary: loss 0.6650, accuracy 0.555\n",
      "epoch  5, step 51500. Train summary: loss 0.6654, accuracy 0.515\n",
      "epoch  5, step 51750. Train summary: loss 0.6664, accuracy 0.488\n",
      "epoch  5, step 52000. Train summary: loss 0.6688, accuracy 0.535\n",
      "Validation accuracy 0.515\n",
      "\n",
      "epoch  5, step 52250. Train summary: loss 0.6548, accuracy 0.585\n",
      "epoch  5, step 52500. Train summary: loss 0.6663, accuracy 0.540\n",
      "epoch  5, step 52750. Train summary: loss 0.6608, accuracy 0.502\n",
      "epoch  5, step 53000. Train summary: loss 0.6687, accuracy 0.530\n",
      "Validation accuracy 0.515\n",
      "\n",
      "epoch  5, step 53250. Train summary: loss 0.6697, accuracy 0.493\n",
      "epoch  5, step 53500. Train summary: loss 0.6661, accuracy 0.548\n",
      "epoch  5, step 53750. Train summary: loss 0.6727, accuracy 0.505\n",
      "epoch  5, step 54000. Train summary: loss 0.6739, accuracy 0.480\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  5, step 54250. Train summary: loss 0.6552, accuracy 0.495\n",
      "epoch  5, step 54500. Train summary: loss 0.6587, accuracy 0.535\n",
      "epoch  5, step 54750. Train summary: loss 0.6635, accuracy 0.540\n",
      "epoch  5, step 55000. Train summary: loss 0.6541, accuracy 0.585\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  5, step 55250. Train summary: loss 0.6638, accuracy 0.490\n",
      "epoch  5, step 55500. Train summary: loss 0.6682, accuracy 0.517\n",
      "epoch  5, step 55750. Train summary: loss 0.6646, accuracy 0.505\n",
      "epoch  5, step 56000. Train summary: loss 0.6447, accuracy 0.525\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  5, step 56250. Train summary: loss 0.6670, accuracy 0.502\n",
      "epoch  5, step 56500. Train summary: loss 0.6685, accuracy 0.520\n",
      "epoch  5, step 56750. Train summary: loss 0.6698, accuracy 0.567\n",
      "epoch  5, step 57000. Train summary: loss 0.6667, accuracy 0.562\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  5, step 57250. Train summary: loss 0.6567, accuracy 0.558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  5, step 57500. Train summary: loss 0.6670, accuracy 0.533\n",
      "epoch  5, step 57750. Train summary: loss 0.6532, accuracy 0.535\n",
      "epoch  5, step 58000. Train summary: loss 0.6596, accuracy 0.590\n",
      "Validation accuracy 0.514\n",
      "\n",
      "epoch  5, step 58250. Train summary: loss 0.6645, accuracy 0.512\n",
      "epoch  5, step 58500. Train summary: loss 0.6620, accuracy 0.510\n",
      "epoch  5, step 58750. Train summary: loss 0.6646, accuracy 0.515\n",
      "epoch  5, step 59000. Train summary: loss 0.6676, accuracy 0.520\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  5, step 59250. Train summary: loss 0.6588, accuracy 0.520\n",
      "epoch  5, step 59500. Train summary: loss 0.6725, accuracy 0.520\n",
      "epoch  5, step 59750. Train summary: loss 0.6825, accuracy 0.455\n",
      "epoch  5, step 60000. Train summary: loss 0.6631, accuracy 0.490\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  5, step 60250. Train summary: loss 0.6780, accuracy 0.500\n",
      "epoch  5, step 60500. Train summary: loss 0.6768, accuracy 0.498\n",
      "epoch  5, step 60750. Train summary: loss 0.6711, accuracy 0.562\n",
      "epoch  5, step 61000. Train summary: loss 0.6585, accuracy 0.530\n",
      "Validation accuracy 0.512\n",
      "\n",
      "epoch  5, step 61250. Train summary: loss 0.6575, accuracy 0.548\n",
      "epoch  5, step 61500. Train summary: loss 0.6724, accuracy 0.470\n",
      "epoch  5, step 61750. Train summary: loss 0.6587, accuracy 0.505\n",
      "epoch  5, step 62000. Train summary: loss 0.6690, accuracy 0.502\n",
      "Validation accuracy 0.513\n",
      "\n",
      "epoch  5, step 62250. Train summary: loss 0.6548, accuracy 0.535\n",
      "epoch  5, step 62500. Train summary: loss 0.6676, accuracy 0.558\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "epochs = 5\n",
    "vary_lengths = False\n",
    "\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for inputs, targets in train_loader:\n",
    "        lengths = adjust_lengths(vary_lengths, inputs, targets)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, predictions = model(inputs, lengths)\n",
    "\n",
    "        # BCEWithLogitsLoss will do the activation\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        accuracy = ((predictions > 0.5) == (targets > 0.5)).type(torch.FloatTensor).mean()\n",
    "        train_accs.append((step, accuracy))\n",
    "\n",
    "        if step % 250 == 0:\n",
    "            #Note: If you receive an error at the following line, you are not using a Python 3.6.x kernel\n",
    "            print(f'epoch {epoch:2d}, step {step:5d}. Train summary: loss {loss.item():.{4}f}, accuracy {accuracy:.{3}f}')\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            valid_accuracy = evaluate(max_bits, vary_lengths, device, model)\n",
    "            valid_accs.append((step, valid_accuracy))\n",
    "            print(f'Validation accuracy {valid_accuracy:.{3}f}\\n')\n",
    "            if valid_accuracy == 1.0:\n",
    "                # stop early\n",
    "                print('Stop early: valid_accuracy == 1.0')\n",
    "                break\n",
    "    if valid_accuracy == 1.0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(train_accs)[:,0], np.array(train_accs)[:,1], label='Training accuracy')\n",
    "plt.plot(np.array(valid_accs)[:,0], np.array(valid_accs)[:,1], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    "1. The code provided uses fixed length sequences. Save your results, change `vary_lengths` to `True`, and run it again. Does the RNN still succeed? What explains the difference?\n",
    "1. Try and change the model architecture - e.g. number of units or number of layers. How does that affect performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Assignments\n",
    "1. Compare RNN and LSTM. Replace `torch.nn.RNN` with `torch.nn.LSTM` in order to use an LSTM instead. Compare the results of the LSTM and the RNN. Can you see a difference?\n",
    "1. Try and **remove teacher forcing**. This will require you to change the data generator `get_random_bits_parity` to use `np.sum` rather than `np.cumsum`. You also need to change some things in the training loop - use the error mesages to guide you.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
